{"cells":[{"cell_type":"markdown","source":"### Collecting data from web-based sources\n\nApart from data libraries there are Two Main data Sources:\n\n1) querying an API (the majority of which are web-based, these days);\n\n2) scraping data from a web page.","metadata":{"tags":[],"cell_id":"00001-88601fec-ecbe-4434-bbcd-f3970ce532b0","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00002-75e97ffc-edf1-4b14-a857-6877448fc0dd","output_cleared":false,"source_hash":"c195679e","execution_millis":297,"deepnote_to_be_reexecuted":false,"execution_start":1610070828473,"deepnote_cell_type":"code"},"source":"import requests\nresponse = requests.get(\"http://www.cmu.edu\")\n\nprint(\"Status Code:\", response.status_code)\nprint(\"Headers:\", response.headers)","execution_count":null,"outputs":[{"name":"stdout","text":"Status Code: 200\nHeaders: {'Date': 'Fri, 08 Jan 2021 01:53:48 GMT', 'Server': 'Apache', 'x-xss-protection': '1; mode=block', 'x-content-type-options': 'nosniff, nosniff', 'x-frame-options': 'SAMEORIGIN', 'Vary': 'Referer', 'Accept-Ranges': 'bytes', 'Cache-Control': 'max-age=7200, must-revalidate', 'Expires': 'Fri, 08 Jan 2021 03:53:48 GMT', 'Keep-Alive': 'timeout=5, max=500', 'Connection': 'Keep-Alive', 'Transfer-Encoding': 'chunked', 'Content-Type': 'text/html'}\n","output_type":"stream"}]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00003-cd30b772-0e1f-498f-8195-baec039f6fde","output_cleared":false,"source_hash":"d8281640","execution_millis":652,"deepnote_to_be_reexecuted":false,"execution_start":1610070828775,"deepnote_cell_type":"code"},"source":"## Another Example\n\nparams = {\"query\": \"python download url content\", \"source\":\"chrome\"}\nresponse = requests.get(\"http://www.google.com/search\", params=params)\nprint(response.status_code)","execution_count":null,"outputs":[{"name":"stdout","text":"200\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### RESTful APIs\n\n\nWhile parsing data in HTML (the format returned by these web queries) is sometimes a necessity, and we’ll discuss it further before, HTML is meant as a format for displaying pages visually, not as the most efficient manner for encoding data. Fortunately, a fair number of web-based data services you will use in practice employ something called REST (Representational State Transfer, but no one uses this term) APIs. We won’t go into detail about REST APIs, but there are a few main feature that are important for our purposes:\n\n1. You call REST APIs using standard HTTP commands: GET, POST, DELETE, PUT. \n2. You will probably see GET and POST used most frequently.\n3. REST servers don’t store state. This means that each time you issue a request, you need to include all relevant information like your account key, etc.\n4. REST calls will usually return information in a nice format, typically JSON (more on this later). The requests library will automatically parse it to return a Python dictionary with the relevant data.\n5. Let’s see how to issue a REST request using the same method as before. We’ll here query my GitHub account to get information. More info about GitHub’s REST API is available at their Developer Site.","metadata":{"tags":[],"cell_id":"00005-c06d0fcb-b920-42a0-b700-47a35c6ae026","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00007-603a4f07-002f-4734-aef0-29903f63a33c","deepnote_to_be_reexecuted":false,"source_hash":"7c7a15b4","execution_start":1610070829434,"execution_millis":65,"deepnote_cell_type":"code"},"source":"#### Get your own at https://github.com/settings/tokens/new\ntoken = \"3125e4430a58c5259a14ddd48157061cdb7055c0\" \nresponse = requests.get(\"https://api.github.com/user\", params={\"access_token\":token})\n\nprint(response.status_code)\nprint(response.headers[\"Content-Type\"])\nprint(response.json().keys())","execution_count":null,"outputs":[{"name":"stdout","text":"401\napplication/json; charset=utf-8\ndict_keys(['message', 'documentation_url'])\n","output_type":"stream"}]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00008-d626788e-cce1-4d50-a032-af69b8b2bf04","deepnote_to_be_reexecuted":false,"source_hash":"b623e53d","execution_start":1610070829502,"execution_millis":1,"deepnote_cell_type":"code"},"source":"### Web Scrapping ","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{"tags":[],"cell_id":"00012-e902b5d0-b35b-4355-8e34-3f967b2b1132","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"","metadata":{"tags":[],"cell_id":"00015-57cf772d-84a9-4f92-9045-8078b288a64b","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"### E-Commerce Web Scrapping ","metadata":{"tags":[],"cell_id":"00008-268570ea-bffe-48ec-95d5-49724659ae82","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"Instead of using modern libraries, we learn the basics of web scrapping\n\n\nWe collect items being sold on an e-commerce website like:  www.katespade.com and items on sale from: www.katespade.com\n\nSee Crawler Script: /web_crawler.py","metadata":{"tags":[],"cell_id":"00009-f99d978b-f8be-402d-8ec3-b9f3ae4ef34c","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"","metadata":{"tags":[],"cell_id":"00010-0c7abf1f-336f-4708-9cc6-88a22f5a03dc","deepnote_cell_type":"markdown"}}],"nbformat":4,"nbformat_minor":2,"metadata":{"orig_nbformat":2,"deepnote_notebook_id":"20d6d947-aeee-4fd9-86d5-5996ee58ae60","deepnote_execution_queue":[]}}