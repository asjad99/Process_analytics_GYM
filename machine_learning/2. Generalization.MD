## Generalziation 

Our Major goal of machine learning is to generalize beyond the examples seen during training. 

Memorisation is not a good strategy for learners because we often see only a fraction of examples we can will encounter in real world settings. 

Luckily induction works pretty well, which means by taking certain assumptions ML algorithms are able to figure out knowledge from a small number of examples.  

*"The ability to perform well on previously unobserved inputs is called generalization".* 

How hard is this objective? well generally speaking the more data we have, the more better chance we have of figuring out the ground truth function 'f'.  Our success also depends on no. of functions that could be ground truth(fewer the better). But again, Machine learning is not just about optimization where we reduce the training error. what we really care about is the generalization error(or test error), defined as "the expected value of the error on a new input. i.e probability that g disagrees with the ground truth f on the label of a random sample" [2][3].



### How do we know if our models generalizes well? 

In Machine Learning we are interested in finding out How well our model will perform on instances it has never seen before. 

We Divide the dataset into train and test set and the error rate on new cases is called generalisation error. We can get a rough estimate of it by 
evaluating our model on the test set.  


### what is the purpose of validation set?





### no free lunch” theorem: 

According to “no free lunch” theorem, no learner can beat random guessing over all possible functions to be learned.

