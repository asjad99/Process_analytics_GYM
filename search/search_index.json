{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Data Science - A Primer About: These Notes and notebooks are currently 'work in progress'. It's inspired by Open Source Society University , peter norvig's pytudes project , multiple Project's by folks like: Donne_Martin , Dfriends , Chris_albon and many others. Roadmap: It provides an opportunity to document some of my own learnings and acts as a data science road map for self-taught learners out there to learn data science for free Computational notebooks : Computational notebooks are essentially laboratory notebooks for scientific computing. We use Notebooks for Practice(as they are best suited for scientific computing). This is closely related to litrate programming paradigm(as conceived by Don Knuth) This is a long list. see this great article on how to approach it depending on the Career paths you decide to take Topics: Introduction to data Science Data Science Experiment LifeCyle Data Wrangling Tools Exploratory Data Analysis Data Cleaning Data Analytics (Descriptive, Inferential, Predictive, Causal) Importance of Story telling and Case Studies Experimental Design Product Related Analysis Natural Language Processing Decision Making (Probability, Reinforcement Learning) Process Analytics Data Engineering (Large Scale Data Processing) Math for Data Science Project Management for data Science Inspiring data Products Useful datasets Introduction to Data Science Data science organizations help a firm\u2019s leaders make scientific or data-driven decisions to run their business more effectively. Team members collect data, analyze datasets, and suggest hypotheses and actions. Data Science has been around for a while. Even big data has been around for a while(Hubble has been sending us image data and Scientists at CERN have been collecting Tera Bytes to uncover the secrets of the universe). Business recently realised that can extract value out of the data they collect(e.g user data and event data) to make data-informed decisions which replaces the old model of going with the gut instinct, loudest voice, and best argument methods. The insights gathered through this process can help improve existing processes and lower operations cost. Learning 'how to be smart with data' is also bit of an art, that requires curosity, creativity and attention to detail and so on. All of this requires experience and practice. Instead of providing a sophisticated definition of Data Science, We run with this definition for now(by J. Kolter at CMU): Data science = statistics + data collection + data preprocessing + machine learning + visualization + business insights + scientific hypotheses + big data + (etc) Types of data scientists by Michael Hochster: Type A Data Scientist: The A is for Analysis. This type is primarily concerned with making sense of data or working with it in a fairly static way. The Type A Data Scientist is very similar to a statistician (and may be one) but knows all the practical details of working with data that aren\u2019t taught in the statistics curriculum: data cleaning, methods for dealing with very large data sets, visualization, deep knowledge of a particular domain, writing well about data, and so on. Type B Data Scientist: The B is for Building. Type B Data Scientists share some statistical background with Type A, but they are also very strong coders and may be trained software engineers. The Type B Data Scientist is mainly interested in using data \u201cin production.\u201d They build models which interact with users, often serving recommendations (products, people you may know, ads, movies, search results). Inductive vs Deductive Reasoning: \"Data Science supports and encourages shifting between deductive (hypothesis-based) and inductive (pattern-based) reasoning. This is a fundamental change from traditional analytic approaches. Inductive reasoning and exploratory data analysis provide a means to form or refine hypotheses and discover new analytic paths. In fact, to do the discovery of signifcant insights that are the hallmark of Data Science, you must have the tradecraft and the interplay between inductive and deductive reasoning. By actively combining the ability to reason deductively and inductively, Data Science creates an environment where models of reality no longer need to be static and empirically based. Instead, they are constantly tested, updated and improved until better models are found. These concepts are summarized in the figure, The Types of Reason and Their Role in Data Science Tradecraft.\" - Field Guide to data Science read more: - Competing in a Data-Driven World: An executive Guide - Rise of Data Products - Beautiful data Data Science Experiment LifeCyle: Introduction: Every data Science Project starts with a question that is to be answered with data. That means forming the question is an important first step in the process. The second step is finding or generating the data you are going to use to answer that question. With that question solidified and data in hand, the data are then analysed, first by exploring the data and then often by modeling the data, which means using some statisitcal or machine learning techniques to analyse the data and answer your question This is an iterative Process After drawing conclusions from this analysis, the project has to be communicated to others. Details: Data Science analytics are a lot like broccoli \u2013 fractal in nature in both time and construction. Early versions of an analytic follow the same development process as later versions. At any given iteration, the analytic itself is a collection of smaller analytics that often decompose into yet smaller analytics. Decomposing the problem into manageable pieces is the frist step in the analytic selection process. Achieving a desired analytic action often requires combining multiple analytic techniques into a holistic, end-to-end solution. Engineering the complete solution requires that the problem be decomposed into progressively smaller sub-problems. Fractal Analytic Model embodies this approach. At any given stage, the analytic itself is a collection of smaller computations that decompose into yet smaller computations. When the problem is decomposed far enough, only a single analytic technique is needed to achieve the analytic goal. Problem decomposition creates multiple sub-problems, each with their own goals, data, computations, and actions. readmore/references: Data Science Field guide - Booz Allen Hamilton A Recipe for Doing Great Data Science Work Importance of Story telling and Case Studies Context, inferences and models are created by humans and carry with them biases and assumptions. Blindly trusting your analyses is a dangerous thing that can lead to erroneous conclusions. We should try to clearly communite our findings by describing: What problem are we trying to solve and why its intresting? Document your assumptions and make sure they have not introduced bias in your work. Does the approach taken and answers make sense? (we should be Be skeptical of surprise findings and make sure the analysis address the original intent) The goal of your analysis is to tell an actionable story. Its good to see some data science projects and learn from them. In each project, the author had a question they wanted to answer and used data to answer that question. They explored, visualized, and analysed the data. Then, they wrote blog posts to communicate their findings. Take a look to learn more about the topics listed and to see how others work through the data science project process and communicate their results! https://pennmusa.github.io/MUSA_801.io/project_5/index.html https://sharlagelfand.netlify.app/posts/tidying-toronto-open-data/ https://masalmon.eu/2017/11/16/wheretoliveus/ http://varianceexplained.org/r/trump-tweets/ Data Acquizition Note: automate as much as possible so you can easily get fresh data. List the data you need and how much you need. Find and document where you can get that data (identify what kind of data the organization has) Check how much space it take and engineering efforts required (initial assessment). Understand Operational Constraints (e.g. what data is actually available at inference time) Legal/Ethics/Privacy Get access authorizations and check legal obligations proactively identify ethical risks, including how your work could be mis-used by harassers, trolls, authoritarian governments, or for propaganda/disinformation campaigns (and plan how to reduce these risks) identify [potential biases and potential negative feedback loops] Ensure sensitive Information is deleted or protected (e.g anonymized) Data Wrangling Tools and Libraries Data wrangling is about taking a messy or unrefined source of data and turning it into something useful. You begin by seeking out raw data sources and determining their value: How good are they as data sets? How relevant are they to your goal? Is there a better source? Once you\u2019ve parsed and cleaned the data so that the data sets are usable, you can utilize tools and methods (like Python scripts) to help you analyze them and present your findings in a report. This allows you to take data no one would bother looking at and make it both clear and actionable. Learning the tools Notebook Description Code Command-line Learn various unix command line utlities and how they can be used to clean and compute basic statistics blog_post Web Data Collection Learn to collect data available on web (APIs or web scrapping) Notebook numpy_Basics Learn the basics of the library that underpins scientific computing Notebook numpy_linear_algebra Notebook Pandas Basics Learn the basics of pandas Notebook Guides on Python Programming How to think like a CS in python SCIP in python Hitch Hiker's Guide to python python env https://jacobian.org/2018/feb/21/python-environment-2018/ Cheatsheet Mit Hacker tools Data Algorithms Data Analytics: we cover the following topics: Descriptive Analysis / Diagnostic Aaalytics / Data Mining : The goal of descriptive analysis is to describe or summarize a set of data. The goal of diagnostic and data mining is to look for hidden insights, answer questions and so on. Inferential Analysis: The goal of inferential analysis is to use a relatively small sample of data to infer or say something about population at large. Predictive Analytics: The goal of predictive analysis is to use current data to make predictions about future data. Causal Analysis: The caveat to a lot of the analysis we have looked at above is that we can only see correlations and can\u2019t get at the cause of the relationships we observe. Causal analysis fills that gap. the goal of the causal analysis is to see what happens to one variable when we manipulate anohter variable. looking at the cause and effect of a relationship. Descriptive Analysis / Diagnostic Aaalytics / Data Mining Descriptive Analytics is aimed at answering real busiessn questions. Describing the dataset at hand, Discovering insigths and Acting on those insights. It requires finding meaningful patterns, trends and exceptions that are easy to see and interpret for decision makers. The goal of descriptive analysis is to describe or summarize a set of data. Whenever you get a new dataset to examine, this is usually the first kind of analysis you will perform. Descriptive analysis will generate simple summaries about the samples and their measurements. You may be familiar with common descriptive statistics: measures of central tendency (eg: mean, median, mode) or measures of variability (eg: range, standard deviations or variance). This type of analysis is aimed at summarizing your sample \u2013 not for generalizing the results of the analysis to a larger population or trying to make conclusions. Description of data is separated from making interpretations; generalizations and interpretations require additional statistical steps. Some examples of purely descriptive analysis can be seen in censuses. Here, the government collects a series of measurements on all of the country\u2019s citizens, which can then be summarized. Here, you are being shown the age distribution in the US, stratified by sex. The goal of this is just to describe the distribution. There is no inferences about what this means or predictions on how the data might trend in the future. It is just to show you a summary of the data collected. Framing a question Statistics knowledge (covered earlier in EDA) Projects: Notebook Description Code Data Visualization Analyse Stackoverflow data using visaluzation techniques Notebook Taxi data Analysis Mobility behaviour Analysis of rome taxi driver Repo Read More: Guide for Common Data Types and Formats Statistics for Social Sciences Edward Tufte's Books Economist What's going on in this graph A network of science: 150 years of Nature papers Inferential analysis (with Statistics) The goal of inferential analysis is to use a relatively small sample of data to infer or say something about population at large. Notebook Description Code Inferential Analysis Notebook Useful resoruces: - Statistics with Python - Experimental Design and Analysis: https://online-learning.harvard.edu/course/data-science-inference-and-modeling?delta=0 https://www.amazon.com/Statistics-Course-Pack-Set-Op/dp/1138838349/ref=sr_1_1?keywords=Statistics+Plain&qid=1572967665&sr=8-1 https://greenteapress.com/thinkstats/thinkstats.pdf Causal analysis The caveat to a lot of the analysis we have looked at above is that we can only see correlations and can\u2019t get at the cause of the relationships we observe. Causal analysis fills that gap. the goal of the causal analysis is to see what happens to one variable when we manipulate anohter variable. Looking at the cause and effect of a relationship. https://ff13.fastforwardlabs.com/ Experimental Design Randomized Control Hypothesis Testing A/B Testing As a Data Scientist, you get to establish causality (something really hard to do with observational data) by running actual randomized, controlled experiments. At Twitter, \u201cIt\u2019s rare for a day to go by without running at least one experiment\u201d \u2014 Alex Roetter, VP of Engineering. A/B testing is ingrained in our DNA and our product development cycle. Here is the typical process of running a A/B test: Gather Samples -> Assign Buckets -> Apply Treatments -> Measure Outcomes -> Make Comparisons. Hypothesis Testing: Statistical test, p-values, statistical significance, power, effect size, multiple testing Useful Guide: - Experimental Design and Analysis by Howard J. Seltman - http://www.datasciencecourse.org/slides/hypothesis_testing.pdf - So, you need a statistically significant sample? - Experiments at Airbnb - When should A/B testing not to be trusted when making decisions Product Related Analysis Examples of Analysis at twitter: Push Notification Analysis \u2014 How many users are eligible for push notifications? across user segment? across clients? What are the tap rates of different push notification types? SMS Delivery Rates \u2014 How do we calculate Twitter\u2019s SMS delivery rates across different carriers? Are our delivery rates in emerging countries poorer? How can we make them better? Multiple Accounts \u2014 Why do certain countries have a higher ratio of multiple accounts? What drive people to create multiple account? Generating insights through product analysis is an iterative process. It requires challenging the questions being asked, understanding the business context, and figuring out the right dataset to answer the questions. Over time, you will become an expert in where the data lives and what they mean. You will get better at estimating how much time it will take to carry out an analysis. More importantly, you will slowly move from a reactive state to proactive state and start suggesting interesting analyses that product leaders might not think of, because they don\u2019t know the data exists or that disparately different data sources can be complementary and combined in a particular way. Gather Samples \u2014 How many samples do we need? How many users should go into each bucket? Can we ensure that the experiment will have sufficient power? Assign Buckets \u2014 Who are eligible to be in the experiments? and where in the code should we start assigning buckets and showing treatments? Would the placement introduce data dilution (i.e. some users are assigned to treatment but never see it)? Apply Treatment \u2014 Are there any other teams in the organization running experiments that are competing for the same real estate in the app? How do we deal with experiment collision and ensure our data is not contaminated? Measure Outcome \u2014 What is the hypothesis of the experiment? What are the success and failure metrics of this experiment? Can we track them? and How? What additional logging do we need to add? Make Comparisons \u2014 Suppose we see that the # of users who logged-in increase dramatically, is it due to noise? How do we know if the results are statistically significant? Even if it is, is it practically significant? Implementing models: Set up the infrastructure, aggregate and prepare the data, and incorporate domain expert knowledge. Try different analytic techniques and models on subsets of the data. Evaluate the models, refine, evaluate again, and select a model. Do something with your models and results \u2013 deploy the models to inform, inspire action, and act. Evaluate the business results to improve the overall product. Predictive Analytics with Machine Learning See Machine Learning GYM Reinforcement Learning: See Reinforcement Learning GYM Process Analytics See Reinforcement Learning GYM Data Engineering See Data Engineering Math for Data Science https://www.overleaf.com/project/60321e0c4ca63c16ec680cde Data science rests on a foundation of mathematics, particularly statistics and linear algebra. It is important to understand this material on an intuitive level: why these concepts were developed, how they are useful, and when they work best. e.g develop statistical reasoning is a core goal. Algorithm Description Code Sets Graph Theory Linear Algebra Probability http://www.datasciencecourse.org/notes/probability/ Statistics Project Management Data Science is more than feature engineering, training models. XOR build applications and deploy models. Its involves: meeting client, understanding business problem, write code to connect to data sources, get the data, clean it, set up training environments, train models, set up serving environments, deploy models, write the application (web, mobile). Data Science supports and encourages shifting between deductive (hypothesis-based) and inductive (pattern-based) reasoning. Tis is a fundamental change from traditional analytic approaches. Inductive reasoning and exploratory data analysis provide a means to form or refine hypotheses and discover new analytic paths. In fact, to do the discovery of signifcant insights that are the hallmark of Data Science, you must have the tradecraft and the interplay between inductive and deductive reasoning. By actively combining the ability to reason deductively and inductively, Data Science creates an environment where models of reality no longer need to be static and empirically based. Instead, they are constantly tested, updated and improved until better models are found. These concepts are summarized in the figure, The Types of Reason and Their Role in Data Science Tradecraft. The diferences between Data Science and traditional analytic approaches do not end at seamless shifting between deductive and inductive reasoning. Data Science offers a distinctly different perspective than capabilities such as Business Intelligence. Data Science should not replace Business Intelligence functions within an organization, however. Te two capabilities are additive and complementary, each offering a necessary view of business operations and the operating environment. Te fgure, Business Intelligence and Data Science \u2013 A Comparison, highlights the diferences between the two capabilities. Key contrasts include: Data Science Projects Pipeline: Data Collection \u2192 Data Processing \u2192 Exploration/visualization \u2192 analysis/machine learning \u2192 insight/policy decisions Data Science Maturity within an Organization: We use the Data Science Maturity Model as a common framework for describing the maturity progression and components that make up a Data Science capability. Tis framework can be applied to an organization\u2019s Data Science capability or even to the maturity of a specifc solution, namely a data product. At each stage of maturity, powerful insight can be gained. Building Your Data Science Team: A critical component to any Data Science capability is having the right team. Data Science depends on a diverse set of skills as shown in Te Data Science Venn Diagram. Computers provide the environment in which data-driven hypotheses are tested, and as such, computer science is necessary for data manipulation and processing. Mathematics provides the theoretical structure in which Data Science problems are examined. A rich background in statistics, geometry, linear algebra, and calculus are all important to understand the basis for many algorithms and tools. Finally, domain expertise contributes to an understanding of what problems actually need to be solved, what kind of data exists in the domain, and how the problem space may be instrumented and measured. Referenes and Further Readings: Field Guide to data science https://wolfpaulus.com/wp-content/uploads/2017/05/field-guide-to-data-science.pdf Executive Data Science Specialization AI Product manager nanodegree Creating a data driven organization Managing Data Science Teams and Projects Interviews: http://treycausey.com/data_science_interviews.html Inspiring Data Products https://github.com/asjad99/data_products Useful datasets https://www.kaggle.com/asjad99 https://github.com/beamandrew/medical-data In the past few decades, there has been an explosion in data that does not have any explicit semantics attached to it. This data is generated by both humans and machines. Most of this data is not easily machine-processable; for example, images, text, video (as opposed to carefully curated data in a knowledge- or data-base). This has given rise to a huge industry that applies AI techniques to get usable information from such enormous data. This field of applying techniques derived from AI to large volumes of data goes by names such as \u201cdata mining,\u201d \u201cbig data,\u201d \u201canalytics,\u201d etc. This field is too vast to even moderately cover in the present article, but we note that there is no full agreement on what constitutes such a \u201cbig-data\u201d problem. One definition, from Madden (2012), is that big data differs from traditional machine-processable data in that it is too big (for most of the existing state-of-the-art hardware), too quick (generated at a fast rate, e.g. online email transactions), or too hard. It is in the too-hard part that AI techniques work quite well. While this universe is quite varied, we use the Watson\u2019s system later in this article as an AI-relevant exemplar. As we will see later, while most of this new explosion is powered by learning, it isn\u2019t entirely limited to just learning. This bloom in learning algorithms has been supported by both a resurgence in neurocomputational techniques and probabilistic techniques. https://plato.stanford.edu/entries/artificial-intelligence/# \u201cEvery statistician knows that a large, relevant sample size is their best friend. What are the three largest, most relevant sample sizes for identifying universal principles? Bucket number one is inorganic systems, which are 13.7 billion years in size. It\u2019s all the laws of math and physics, the entire physical universe. Bucket number two is organic systems, 3.5 billion years of biology on Earth. And bucket number three is human history, you can pick your own number, I picked 20,000 years of recorded human behavior. Those are the three largest sample sizes we can access and the most relevant.\u201d Perched atop the Cerro Pach\u00f3n ridge in the Chilean Andes is a building site that will eventually become the Large Synoptic Survey Telescope (LSST). When it comes online in 2022, the telescope will generate terabytes of data each night as it surveys the southern skies automatically. And to crunch those data, astronomers will use a familiar and increasingly popular tool: the Jupyter notebook. \"In Short, the new breed of scientist must be a broadly-trained expert in statistics, in computing, in algorithmic building, in software design.\"","title":"Home"},{"location":"#about","text":"These Notes and notebooks are currently 'work in progress'. It's inspired by Open Source Society University , peter norvig's pytudes project , multiple Project's by folks like: Donne_Martin , Dfriends , Chris_albon and many others. Roadmap: It provides an opportunity to document some of my own learnings and acts as a data science road map for self-taught learners out there to learn data science for free Computational notebooks : Computational notebooks are essentially laboratory notebooks for scientific computing. We use Notebooks for Practice(as they are best suited for scientific computing). This is closely related to litrate programming paradigm(as conceived by Don Knuth) This is a long list. see this great article on how to approach it depending on the Career paths you decide to take","title":"About:"},{"location":"#topics","text":"Introduction to data Science Data Science Experiment LifeCyle Data Wrangling Tools Exploratory Data Analysis Data Cleaning Data Analytics (Descriptive, Inferential, Predictive, Causal) Importance of Story telling and Case Studies Experimental Design Product Related Analysis Natural Language Processing Decision Making (Probability, Reinforcement Learning) Process Analytics Data Engineering (Large Scale Data Processing) Math for Data Science Project Management for data Science Inspiring data Products Useful datasets","title":"Topics:"},{"location":"#introduction-to-data-science","text":"Data science organizations help a firm\u2019s leaders make scientific or data-driven decisions to run their business more effectively. Team members collect data, analyze datasets, and suggest hypotheses and actions. Data Science has been around for a while. Even big data has been around for a while(Hubble has been sending us image data and Scientists at CERN have been collecting Tera Bytes to uncover the secrets of the universe). Business recently realised that can extract value out of the data they collect(e.g user data and event data) to make data-informed decisions which replaces the old model of going with the gut instinct, loudest voice, and best argument methods. The insights gathered through this process can help improve existing processes and lower operations cost. Learning 'how to be smart with data' is also bit of an art, that requires curosity, creativity and attention to detail and so on. All of this requires experience and practice. Instead of providing a sophisticated definition of Data Science, We run with this definition for now(by J. Kolter at CMU): Data science = statistics + data collection + data preprocessing + machine learning + visualization + business insights + scientific hypotheses + big data + (etc) Types of data scientists by Michael Hochster: Type A Data Scientist: The A is for Analysis. This type is primarily concerned with making sense of data or working with it in a fairly static way. The Type A Data Scientist is very similar to a statistician (and may be one) but knows all the practical details of working with data that aren\u2019t taught in the statistics curriculum: data cleaning, methods for dealing with very large data sets, visualization, deep knowledge of a particular domain, writing well about data, and so on. Type B Data Scientist: The B is for Building. Type B Data Scientists share some statistical background with Type A, but they are also very strong coders and may be trained software engineers. The Type B Data Scientist is mainly interested in using data \u201cin production.\u201d They build models which interact with users, often serving recommendations (products, people you may know, ads, movies, search results).","title":"Introduction to Data Science"},{"location":"#inductive-vs-deductive-reasoning","text":"\"Data Science supports and encourages shifting between deductive (hypothesis-based) and inductive (pattern-based) reasoning. This is a fundamental change from traditional analytic approaches. Inductive reasoning and exploratory data analysis provide a means to form or refine hypotheses and discover new analytic paths. In fact, to do the discovery of signifcant insights that are the hallmark of Data Science, you must have the tradecraft and the interplay between inductive and deductive reasoning. By actively combining the ability to reason deductively and inductively, Data Science creates an environment where models of reality no longer need to be static and empirically based. Instead, they are constantly tested, updated and improved until better models are found. These concepts are summarized in the figure, The Types of Reason and Their Role in Data Science Tradecraft.\" - Field Guide to data Science read more: - Competing in a Data-Driven World: An executive Guide - Rise of Data Products - Beautiful data","title":"Inductive vs Deductive Reasoning:"},{"location":"#data-science-experiment-lifecyle","text":"","title":"Data Science Experiment LifeCyle:"},{"location":"#introduction","text":"Every data Science Project starts with a question that is to be answered with data. That means forming the question is an important first step in the process. The second step is finding or generating the data you are going to use to answer that question. With that question solidified and data in hand, the data are then analysed, first by exploring the data and then often by modeling the data, which means using some statisitcal or machine learning techniques to analyse the data and answer your question This is an iterative Process After drawing conclusions from this analysis, the project has to be communicated to others.","title":"Introduction:"},{"location":"#details","text":"Data Science analytics are a lot like broccoli \u2013 fractal in nature in both time and construction. Early versions of an analytic follow the same development process as later versions. At any given iteration, the analytic itself is a collection of smaller analytics that often decompose into yet smaller analytics. Decomposing the problem into manageable pieces is the frist step in the analytic selection process. Achieving a desired analytic action often requires combining multiple analytic techniques into a holistic, end-to-end solution. Engineering the complete solution requires that the problem be decomposed into progressively smaller sub-problems. Fractal Analytic Model embodies this approach. At any given stage, the analytic itself is a collection of smaller computations that decompose into yet smaller computations. When the problem is decomposed far enough, only a single analytic technique is needed to achieve the analytic goal. Problem decomposition creates multiple sub-problems, each with their own goals, data, computations, and actions.","title":"Details:"},{"location":"#readmorereferences","text":"Data Science Field guide - Booz Allen Hamilton A Recipe for Doing Great Data Science Work","title":"readmore/references:"},{"location":"#importance-of-story-telling-and-case-studies","text":"Context, inferences and models are created by humans and carry with them biases and assumptions. Blindly trusting your analyses is a dangerous thing that can lead to erroneous conclusions. We should try to clearly communite our findings by describing: What problem are we trying to solve and why its intresting? Document your assumptions and make sure they have not introduced bias in your work. Does the approach taken and answers make sense? (we should be Be skeptical of surprise findings and make sure the analysis address the original intent) The goal of your analysis is to tell an actionable story. Its good to see some data science projects and learn from them. In each project, the author had a question they wanted to answer and used data to answer that question. They explored, visualized, and analysed the data. Then, they wrote blog posts to communicate their findings. Take a look to learn more about the topics listed and to see how others work through the data science project process and communicate their results! https://pennmusa.github.io/MUSA_801.io/project_5/index.html https://sharlagelfand.netlify.app/posts/tidying-toronto-open-data/ https://masalmon.eu/2017/11/16/wheretoliveus/ http://varianceexplained.org/r/trump-tweets/","title":"Importance of Story telling and Case Studies"},{"location":"#data-acquizition","text":"Note: automate as much as possible so you can easily get fresh data. List the data you need and how much you need. Find and document where you can get that data (identify what kind of data the organization has) Check how much space it take and engineering efforts required (initial assessment). Understand Operational Constraints (e.g. what data is actually available at inference time) Legal/Ethics/Privacy Get access authorizations and check legal obligations proactively identify ethical risks, including how your work could be mis-used by harassers, trolls, authoritarian governments, or for propaganda/disinformation campaigns (and plan how to reduce these risks) identify [potential biases and potential negative feedback loops] Ensure sensitive Information is deleted or protected (e.g anonymized)","title":"Data Acquizition"},{"location":"#data-wrangling-tools-and-libraries","text":"Data wrangling is about taking a messy or unrefined source of data and turning it into something useful. You begin by seeking out raw data sources and determining their value: How good are they as data sets? How relevant are they to your goal? Is there a better source? Once you\u2019ve parsed and cleaned the data so that the data sets are usable, you can utilize tools and methods (like Python scripts) to help you analyze them and present your findings in a report. This allows you to take data no one would bother looking at and make it both clear and actionable.","title":"Data Wrangling Tools and Libraries"},{"location":"#learning-the-tools","text":"Notebook Description Code Command-line Learn various unix command line utlities and how they can be used to clean and compute basic statistics blog_post Web Data Collection Learn to collect data available on web (APIs or web scrapping) Notebook numpy_Basics Learn the basics of the library that underpins scientific computing Notebook numpy_linear_algebra Notebook Pandas Basics Learn the basics of pandas Notebook","title":"Learning the tools"},{"location":"#guides-on-python-programming","text":"How to think like a CS in python SCIP in python Hitch Hiker's Guide to python python env https://jacobian.org/2018/feb/21/python-environment-2018/ Cheatsheet Mit Hacker tools Data Algorithms","title":"Guides on Python Programming"},{"location":"#data-analytics","text":"we cover the following topics: Descriptive Analysis / Diagnostic Aaalytics / Data Mining : The goal of descriptive analysis is to describe or summarize a set of data. The goal of diagnostic and data mining is to look for hidden insights, answer questions and so on. Inferential Analysis: The goal of inferential analysis is to use a relatively small sample of data to infer or say something about population at large. Predictive Analytics: The goal of predictive analysis is to use current data to make predictions about future data. Causal Analysis: The caveat to a lot of the analysis we have looked at above is that we can only see correlations and can\u2019t get at the cause of the relationships we observe. Causal analysis fills that gap. the goal of the causal analysis is to see what happens to one variable when we manipulate anohter variable. looking at the cause and effect of a relationship.","title":"Data Analytics:"},{"location":"#descriptive-analysis-diagnostic-aaalytics-data-mining","text":"Descriptive Analytics is aimed at answering real busiessn questions. Describing the dataset at hand, Discovering insigths and Acting on those insights. It requires finding meaningful patterns, trends and exceptions that are easy to see and interpret for decision makers. The goal of descriptive analysis is to describe or summarize a set of data. Whenever you get a new dataset to examine, this is usually the first kind of analysis you will perform. Descriptive analysis will generate simple summaries about the samples and their measurements. You may be familiar with common descriptive statistics: measures of central tendency (eg: mean, median, mode) or measures of variability (eg: range, standard deviations or variance). This type of analysis is aimed at summarizing your sample \u2013 not for generalizing the results of the analysis to a larger population or trying to make conclusions. Description of data is separated from making interpretations; generalizations and interpretations require additional statistical steps. Some examples of purely descriptive analysis can be seen in censuses. Here, the government collects a series of measurements on all of the country\u2019s citizens, which can then be summarized. Here, you are being shown the age distribution in the US, stratified by sex. The goal of this is just to describe the distribution. There is no inferences about what this means or predictions on how the data might trend in the future. It is just to show you a summary of the data collected. Framing a question Statistics knowledge (covered earlier in EDA)","title":"Descriptive Analysis / Diagnostic Aaalytics / Data Mining"},{"location":"#projects","text":"Notebook Description Code Data Visualization Analyse Stackoverflow data using visaluzation techniques Notebook Taxi data Analysis Mobility behaviour Analysis of rome taxi driver Repo Read More: Guide for Common Data Types and Formats Statistics for Social Sciences Edward Tufte's Books Economist What's going on in this graph A network of science: 150 years of Nature papers","title":"Projects:"},{"location":"#inferential-analysis-with-statistics","text":"The goal of inferential analysis is to use a relatively small sample of data to infer or say something about population at large. Notebook Description Code Inferential Analysis Notebook","title":"Inferential analysis (with Statistics)"},{"location":"#useful-resoruces","text":"- Statistics with Python - Experimental Design and Analysis: https://online-learning.harvard.edu/course/data-science-inference-and-modeling?delta=0 https://www.amazon.com/Statistics-Course-Pack-Set-Op/dp/1138838349/ref=sr_1_1?keywords=Statistics+Plain&qid=1572967665&sr=8-1 https://greenteapress.com/thinkstats/thinkstats.pdf","title":"Useful resoruces:"},{"location":"#causal-analysis","text":"The caveat to a lot of the analysis we have looked at above is that we can only see correlations and can\u2019t get at the cause of the relationships we observe. Causal analysis fills that gap. the goal of the causal analysis is to see what happens to one variable when we manipulate anohter variable. Looking at the cause and effect of a relationship. https://ff13.fastforwardlabs.com/","title":"Causal analysis"},{"location":"#experimental-design","text":"","title":"Experimental Design"},{"location":"#randomized-control","text":"","title":"Randomized Control"},{"location":"#hypothesis-testing","text":"","title":"Hypothesis Testing"},{"location":"#ab-testing","text":"As a Data Scientist, you get to establish causality (something really hard to do with observational data) by running actual randomized, controlled experiments. At Twitter, \u201cIt\u2019s rare for a day to go by without running at least one experiment\u201d \u2014 Alex Roetter, VP of Engineering. A/B testing is ingrained in our DNA and our product development cycle. Here is the typical process of running a A/B test: Gather Samples -> Assign Buckets -> Apply Treatments -> Measure Outcomes -> Make Comparisons. Hypothesis Testing: Statistical test, p-values, statistical significance, power, effect size, multiple testing Useful Guide: - Experimental Design and Analysis by Howard J. Seltman - http://www.datasciencecourse.org/slides/hypothesis_testing.pdf - So, you need a statistically significant sample? - Experiments at Airbnb - When should A/B testing not to be trusted when making decisions","title":"A/B Testing"},{"location":"#product-related-analysis","text":"Examples of Analysis at twitter: Push Notification Analysis \u2014 How many users are eligible for push notifications? across user segment? across clients? What are the tap rates of different push notification types? SMS Delivery Rates \u2014 How do we calculate Twitter\u2019s SMS delivery rates across different carriers? Are our delivery rates in emerging countries poorer? How can we make them better? Multiple Accounts \u2014 Why do certain countries have a higher ratio of multiple accounts? What drive people to create multiple account? Generating insights through product analysis is an iterative process. It requires challenging the questions being asked, understanding the business context, and figuring out the right dataset to answer the questions. Over time, you will become an expert in where the data lives and what they mean. You will get better at estimating how much time it will take to carry out an analysis. More importantly, you will slowly move from a reactive state to proactive state and start suggesting interesting analyses that product leaders might not think of, because they don\u2019t know the data exists or that disparately different data sources can be complementary and combined in a particular way. Gather Samples \u2014 How many samples do we need? How many users should go into each bucket? Can we ensure that the experiment will have sufficient power? Assign Buckets \u2014 Who are eligible to be in the experiments? and where in the code should we start assigning buckets and showing treatments? Would the placement introduce data dilution (i.e. some users are assigned to treatment but never see it)? Apply Treatment \u2014 Are there any other teams in the organization running experiments that are competing for the same real estate in the app? How do we deal with experiment collision and ensure our data is not contaminated? Measure Outcome \u2014 What is the hypothesis of the experiment? What are the success and failure metrics of this experiment? Can we track them? and How? What additional logging do we need to add? Make Comparisons \u2014 Suppose we see that the # of users who logged-in increase dramatically, is it due to noise? How do we know if the results are statistically significant? Even if it is, is it practically significant? Implementing models: Set up the infrastructure, aggregate and prepare the data, and incorporate domain expert knowledge. Try different analytic techniques and models on subsets of the data. Evaluate the models, refine, evaluate again, and select a model. Do something with your models and results \u2013 deploy the models to inform, inspire action, and act. Evaluate the business results to improve the overall product.","title":"Product Related Analysis"},{"location":"#predictive-analytics-with-machine-learning","text":"See Machine Learning GYM","title":"Predictive Analytics with Machine Learning"},{"location":"#reinforcement-learning","text":"See Reinforcement Learning GYM","title":"Reinforcement Learning:"},{"location":"#process-analytics","text":"See Reinforcement Learning GYM","title":"Process Analytics"},{"location":"#data-engineering","text":"See Data Engineering","title":"Data Engineering"},{"location":"#math-for-data-science","text":"https://www.overleaf.com/project/60321e0c4ca63c16ec680cde Data science rests on a foundation of mathematics, particularly statistics and linear algebra. It is important to understand this material on an intuitive level: why these concepts were developed, how they are useful, and when they work best. e.g develop statistical reasoning is a core goal. Algorithm Description Code Sets Graph Theory Linear Algebra Probability http://www.datasciencecourse.org/notes/probability/ Statistics","title":"Math for Data Science"},{"location":"#project-management","text":"Data Science is more than feature engineering, training models. XOR build applications and deploy models. Its involves: meeting client, understanding business problem, write code to connect to data sources, get the data, clean it, set up training environments, train models, set up serving environments, deploy models, write the application (web, mobile). Data Science supports and encourages shifting between deductive (hypothesis-based) and inductive (pattern-based) reasoning. Tis is a fundamental change from traditional analytic approaches. Inductive reasoning and exploratory data analysis provide a means to form or refine hypotheses and discover new analytic paths. In fact, to do the discovery of signifcant insights that are the hallmark of Data Science, you must have the tradecraft and the interplay between inductive and deductive reasoning. By actively combining the ability to reason deductively and inductively, Data Science creates an environment where models of reality no longer need to be static and empirically based. Instead, they are constantly tested, updated and improved until better models are found. These concepts are summarized in the figure, The Types of Reason and Their Role in Data Science Tradecraft. The diferences between Data Science and traditional analytic approaches do not end at seamless shifting between deductive and inductive reasoning. Data Science offers a distinctly different perspective than capabilities such as Business Intelligence. Data Science should not replace Business Intelligence functions within an organization, however. Te two capabilities are additive and complementary, each offering a necessary view of business operations and the operating environment. Te fgure, Business Intelligence and Data Science \u2013 A Comparison, highlights the diferences between the two capabilities. Key contrasts include:","title":"Project Management"},{"location":"#_1","text":"Data Science Projects Pipeline: Data Collection \u2192 Data Processing \u2192 Exploration/visualization \u2192 analysis/machine learning \u2192 insight/policy decisions","title":""},{"location":"#data-science-maturity-within-an-organization","text":"We use the Data Science Maturity Model as a common framework for describing the maturity progression and components that make up a Data Science capability. Tis framework can be applied to an organization\u2019s Data Science capability or even to the maturity of a specifc solution, namely a data product. At each stage of maturity, powerful insight can be gained.","title":"Data Science Maturity within an Organization:"},{"location":"#building-your-data-science-team","text":"A critical component to any Data Science capability is having the right team. Data Science depends on a diverse set of skills as shown in Te Data Science Venn Diagram. Computers provide the environment in which data-driven hypotheses are tested, and as such, computer science is necessary for data manipulation and processing. Mathematics provides the theoretical structure in which Data Science problems are examined. A rich background in statistics, geometry, linear algebra, and calculus are all important to understand the basis for many algorithms and tools. Finally, domain expertise contributes to an understanding of what problems actually need to be solved, what kind of data exists in the domain, and how the problem space may be instrumented and measured.","title":"Building Your Data Science Team:"},{"location":"#referenes-and-further-readings","text":"Field Guide to data science https://wolfpaulus.com/wp-content/uploads/2017/05/field-guide-to-data-science.pdf Executive Data Science Specialization AI Product manager nanodegree Creating a data driven organization Managing Data Science Teams and Projects Interviews: http://treycausey.com/data_science_interviews.html","title":"Referenes and Further Readings:"},{"location":"#inspiring-data-products","text":"https://github.com/asjad99/data_products","title":"Inspiring Data Products"},{"location":"#useful-datasets","text":"https://www.kaggle.com/asjad99 https://github.com/beamandrew/medical-data In the past few decades, there has been an explosion in data that does not have any explicit semantics attached to it. This data is generated by both humans and machines. Most of this data is not easily machine-processable; for example, images, text, video (as opposed to carefully curated data in a knowledge- or data-base). This has given rise to a huge industry that applies AI techniques to get usable information from such enormous data. This field of applying techniques derived from AI to large volumes of data goes by names such as \u201cdata mining,\u201d \u201cbig data,\u201d \u201canalytics,\u201d etc. This field is too vast to even moderately cover in the present article, but we note that there is no full agreement on what constitutes such a \u201cbig-data\u201d problem. One definition, from Madden (2012), is that big data differs from traditional machine-processable data in that it is too big (for most of the existing state-of-the-art hardware), too quick (generated at a fast rate, e.g. online email transactions), or too hard. It is in the too-hard part that AI techniques work quite well. While this universe is quite varied, we use the Watson\u2019s system later in this article as an AI-relevant exemplar. As we will see later, while most of this new explosion is powered by learning, it isn\u2019t entirely limited to just learning. This bloom in learning algorithms has been supported by both a resurgence in neurocomputational techniques and probabilistic techniques. https://plato.stanford.edu/entries/artificial-intelligence/# \u201cEvery statistician knows that a large, relevant sample size is their best friend. What are the three largest, most relevant sample sizes for identifying universal principles? Bucket number one is inorganic systems, which are 13.7 billion years in size. It\u2019s all the laws of math and physics, the entire physical universe. Bucket number two is organic systems, 3.5 billion years of biology on Earth. And bucket number three is human history, you can pick your own number, I picked 20,000 years of recorded human behavior. Those are the three largest sample sizes we can access and the most relevant.\u201d Perched atop the Cerro Pach\u00f3n ridge in the Chilean Andes is a building site that will eventually become the Large Synoptic Survey Telescope (LSST). When it comes online in 2022, the telescope will generate terabytes of data each night as it surveys the southern skies automatically. And to crunch those data, astronomers will use a familiar and increasingly popular tool: the Jupyter notebook. \"In Short, the new breed of scientist must be a broadly-trained expert in statistics, in computing, in algorithmic building, in software design.\"","title":"Useful datasets"}]}